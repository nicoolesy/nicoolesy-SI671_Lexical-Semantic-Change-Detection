{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "from gensim.models.word2vec import PathLineSentences\n",
    "from collections import defaultdict\n",
    "\n",
    "# read in two corpus c1=old_corpus, c2=modern_corpus by using PathLineSentences\n",
    "c1 = PathLineSentences('/Users/nicolechen/UMSI/FALL23/SI671/final_project/trail/trial_data_public/corpora/german/corpus1/corpus1.txt')\n",
    "c2 = PathLineSentences('/Users/nicolechen/UMSI/FALL23/SI671/final_project/trail/trial_data_public/corpora/german/corpus2/corpus2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyize frequent words in c1 and save word frequency above 40 as old_freq(list format) \n",
    "freqs_w1 = defaultdict(int)\n",
    "for sentence in c1:\n",
    "    for word in sentence:\n",
    "        freqs_w1[word] = freqs_w1[word] + 1\n",
    "freqs_w1_old = pd.Series(freqs_w1).sort_values(ascending=False)\n",
    "old_freq = freqs_w1_old[freqs_w1_old >= 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyize frequent words in c2 and save word frequency above 40 as old_freq(list format) \n",
    "freqs_w2 = defaultdict(int)\n",
    "for sentence in c2:\n",
    "    for word in sentence:\n",
    "        freqs_w2[word] = freqs_w2[word] + 1\n",
    "freqs_w2_modern = pd.Series(freqs_w2).sort_values(ascending=False)\n",
    "modern_freq = freqs_w2_modern[freqs_w2_modern >= 73]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train c1 into Word2Vec model with presetting parameters\n",
    "w1 = gensim.models.Word2Vec(c1,\n",
    "                            sg=1,\n",
    "                            hs=0,\n",
    "                            negative=5,\n",
    "                            sample=0.001,\n",
    "                            vector_size=100, window=10, min_count=20, epochs=5, workers=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train c2 into Word2Vec model with presetting parameters\n",
    "w2 = gensim.models.Word2Vec(c2,\n",
    "                            sg=1,\n",
    "                            hs=0,\n",
    "                            negative=5,\n",
    "                            sample=0.001,\n",
    "                            vector_size=100, window=10, min_count=0, epochs=5, workers=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract common word from modern and old corpus frequent words\n",
    "common_word = sorted(list(set(modern_freq.keys()).intersection(old_freq.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip common_word and save as w1_words and w2_words\n",
    "w1_words, w1_matrix = zip(*[(word, w1.wv[word]) for word in common_word])\n",
    "w2_words, w2_martix = zip(*[(word, w2.wv[word]) for word in common_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim.models as intersected_word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# get the common word list from the two models (I did this in the script above)\n",
    "common_words = sorted(list(set(modern_freq.keys()).intersection(old_freq.keys())))\n",
    "\n",
    "# get key, value pairs of index to key for each model to prepare for reverse lookup later\n",
    "old_idx2key = {num: word for num, word in enumerate(w1.wv.index_to_key)}\n",
    "modern_idx2key = {num: word for num, word in enumerate(w2.wv.index_to_key)}\n",
    "\n",
    "# reverse the key, value pairs to get key to index pairs dictionaries for each model\n",
    "old_key2item = {word: num for num, word in old_idx2key.items()}\n",
    "modern_key2item = {word: num for num, word in modern_idx2key.items()}\n",
    "\n",
    "# spot the common words in each model's vocabulary and get their indices\n",
    "old_shared_indices = [old_key2item[word] for word in common_words]\n",
    "modern_shared_indices = [modern_key2item[word] for word in common_words]\n",
    "\n",
    "# turn original Word2Vec models into KeyedVectors objects to access the underlying vectors\n",
    "old_vecs = w1.wv\n",
    "modern_vecs = w2.wv\n",
    "\n",
    "# extract the common word list vectors from each model\n",
    "old_shared_vecs = old_vecs[old_shared_indices]\n",
    "modern_shared_vecs = modern_vecs[modern_shared_indices]\n",
    "\n",
    "# calculate the transformation matrix for the aligned space using the SVD method based on the formula of othorgonal procrustes problem(the W.T*q-W^(t+1))\n",
    "# refer from LSCD paper\n",
    "m = old_shared_vecs.T @ modern_shared_vecs\n",
    "u, _, v = np.linalg.svd(m)\n",
    "ortho = u @ v\n",
    "\n",
    "# apply otherthogonal procrustes problem to the old model(Q^(t))\n",
    "old_aligened = old_vecs.vectors.dot(ortho)\n",
    "modern_aligened = modern_vecs.vectors.dot(ortho)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{Q}^{\\text{opt}} = \\arg \\underbrace{\\min}_{\\left\\{\\mathbf{Q}^{\\dagger} \\left|{\\mathbf{Q} = {\\mathbf{I}}} \\right. \\right\\}} \\|\\mathbf{W}^{(t)}\\mathbf{Q} - \\mathbf{W}^{(t+1)}\\|_{F}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after alignment, we can get the aligned word list and aligned word vector of old and modern corpus\n",
    "old_ind, old_vec = zip(*[(word, old_aligened[word]) for word in old_shared_indices])\n",
    "modern_ind, modern_vec = zip(*[(word, modern_aligened[word]) for word in modern_shared_indices])\n",
    "\n",
    "old_wordlist = [old_idx2key[idx] for idx in old_ind]\n",
    "modern_wordlist = [modern_idx2key[idx] for idx in modern_ind]\n",
    "\n",
    "# combine the aligned word list and aligned word vector of old and modern corpus\n",
    "old_wordnvec = list(zip(old_wordlist, old_vec))\n",
    "modern_wordnvec = list(zip(modern_wordlist, modern_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4385"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input the target words, seperated by '\\t'\n",
    "target_input = open('/Users/nicolechen/SemEval/LSCDiscovery/starting_kit/testsets/targets_input.tsv', 'r').readlines()\n",
    "targets = [(line.strip().split('\\t')[0], line.strip().split('\\t')[1]) for line in target_input]\n",
    "\n",
    "# get the target word list\n",
    "target_list = []\n",
    "for line in targets:\n",
    "    target_list.append(line[0])\n",
    "\n",
    "# check the workd list len is the same as the target word list\n",
    "len(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manipulate the target word list to get the aligned word list and aligned word vector of old and modern corpus\n",
    "old_cd_vec = [i for i in old_wordnvec if i[0] in target_list]\n",
    "modern_cd_vec = [i for i in modern_wordnvec if i[0] in target_list]\n",
    "\n",
    "# get the aligned word vector of old and modern corpus\n",
    "final_old_vec = [i[1] for i in old_cd_vec]\n",
    "final_modern_vec = [i[1] for i in modern_cd_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(nan, nan)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine as cosine_distance\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# calculate the cosine distance between the aligned word vector of old and modern corpus\n",
    "scores = {}\n",
    "for i in range(len(final_old_vec)):\n",
    "    distance = cosine_distance(final_old_vec[i], final_modern_vec[i])\n",
    "    scores[old_cd_vec[i][0]] = distance\n",
    "  \n",
    "# read the target_scores.txt for spearman correlation\n",
    "with open('/Users/nicolechen/SemEval/LSCDiscovery/dwug_es/target_scores.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    golden_scores = [line.strip().split('\\t') for line in lines]\n",
    "\n",
    "# extract scores from target_scores.txt\n",
    "golden = {}\n",
    "for i in golden_scores:\n",
    "    if i:\n",
    "        golden[i[0]] = i[1]\n",
    "\n",
    "# calculate the spearman correlation\n",
    "corpus = []\n",
    "gold = []\n",
    "for i in golden.keys():\n",
    "    if i in scores.keys():\n",
    "        corpus.append(scores[i])\n",
    "        gold.append(golden[i])\n",
    "spearc, p = spearmanr(corpus, gold, nan_policy='omit')\n",
    "spearc, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d': 0.3907788395881653, 'es': 0.5391152799129486}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
